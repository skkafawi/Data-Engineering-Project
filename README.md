# Data Pipeline Masterpiece: Unveiling the Journey

Welcome to the GitHub repository showcasing our remarkable data engineering journey at WBS Coding School.
In this project, we embarked on an endeavor to transform raw data into valuable insights through a comprehensive data pipeline. This README provides an overview of the project and its key components.

## Project Overview

Our data engineering journey aimed to create an end-to-end data pipeline that encompassed data collection, storage, transformation, and analysis.
We utilized web scraping and APIs to gather data from diverse online sources, harnessed the power of AWS to scale our solution to the cloud, and employed serverless computing with AWS Lambda for enhanced efficiency.

## Key Steps and Highlights

1. **Local Data Pipeline Setup:**
   - Explored web scraping and APIs (using Python) for data collection.
   - Designed a robust database schema for optimal data management.
   - Stored collected data on a local MySQL instance for testing.

2. **Cloud Pipeline Scaling with AWS:**
   - Leveraged Amazon RDS for setting up a cloud database.
   - Transitioned our solution to the cloud using AWS Lambda for serverless execution.
   - Automated the pipeline, orchestrating data collection, processing, and storage.

3. **Conclusion and Future Directions:**
   - Our project exemplified the fusion of theoretical knowledge and practical implementation.
   - Explored the art of sculpting raw data into insights that empower organizations.
   - The GitHub repository contains the full code and details of our data engineering project.
  
## for more explanation feel free to check out my blogpost draft on Medium about this project: [Medium Draft](https://medium.com/p/8bdb70e9d927/edit)https://medium.com/p/8bdb70e9d927/edit)
